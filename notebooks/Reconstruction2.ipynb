{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3prl_path= '../s3prl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(f'{s3prl_path}/transformer/')\n",
    "sys.path.append(f'{s3prl_path}/')\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from torch import nn\n",
    "from model import TransformerModel , TransformerForMaskedAcousticModel , TransformerConfig\n",
    "import transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../scripts')\n",
    "import load_mel\n",
    "from augment import do_aug\n",
    "from mixup import MixUp\n",
    "from preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_audio(y,sr):\n",
    "    librosa.display.waveplot(y, sr=sr)\n",
    "\n",
    "    spec= load_mel.get_spectrogram(y,sr,apply_denoise=False,return_audio=False)\n",
    "    load_mel.plot_feature(spec)\n",
    "    \n",
    "    return Audio(y,rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files= glob.glob('/home/jupyter/rfcx/data/*/*.flac')\n",
    "# audio_files= glob.glob('/home/jupyter/librispeech/LibriSpeech/test-other/1688/142285/*.flac')\n",
    "len(audio_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-leone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path,device='cpu'):\n",
    "\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "    weights = ckpt['Transformer']\n",
    "    config = ckpt['Settings']['Config']\n",
    "\n",
    "    # print(ckpt.keys())\n",
    "\n",
    "\n",
    "    model_config = TransformerConfig(config)\n",
    "    input_dim = config['transformer']['input_dim']\n",
    "    dr= model_config.downsample_rate\n",
    "    hidden_size = model_config.hidden_size\n",
    "\n",
    "    output_attention= False\n",
    "    device= device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    output_dim = input_dim\n",
    "\n",
    "    model = TransformerForMaskedAcousticModel(model_config,\n",
    "                                                    input_dim,\n",
    "                                                    output_dim = output_dim,\n",
    "                                                    output_attentions=output_attention\n",
    "                                                   ).to(device)\n",
    "    \n",
    "    model.Transformer.load_state_dict(ckpt['Transformer'])\n",
    "    model.SpecHead.load_state_dict(ckpt['SpecHead'])\n",
    "\n",
    "    model.eval()\n",
    "    return model, hidden_size, dr, device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformerModel(torch.nn.Module):\n",
    "  \n",
    "    def __init__(self, transformer_model: TransformerForMaskedAcousticModel):\n",
    "        super(CustomTransformerModel,self).__init__()\n",
    "        self.transformer = transformer_model\n",
    "        self.maxlen=3000\n",
    "    \n",
    "    def split(self,inp):\n",
    "        #shape of each input is (batch_size, sequence, mel_features)\n",
    "        #goal is to split the sequence if the sequence length is greater tha maxlen\n",
    "        sequence_length = inp.shape[1]\n",
    "        axes_length= len(inp.shape)\n",
    "        \n",
    "        if sequence_length> self.maxlen:\n",
    "            \n",
    "            sub_sequences= []\n",
    "            num_subseq= sequence_length//self.maxlen\n",
    "            start= 0\n",
    "            \n",
    "            for i in range(1,num_subseq+1):\n",
    "                end= self.maxlen*i\n",
    "                if axes_length==2:\n",
    "                    sub_sequences.append(inp[:, start:end])\n",
    "                else:\n",
    "                    sub_sequences.append(inp[:, start:end, :])\n",
    "                \n",
    "                start=end\n",
    "                \n",
    "            if end<sequence_length:\n",
    "                if axes_length==2:\n",
    "                    sub_sequences.append(inp[:, start:])\n",
    "                else:\n",
    "                    sub_sequences.append(inp[:, start:, :])\n",
    "        \n",
    "            return sub_sequences\n",
    "        else:\n",
    "            return [inp]\n",
    "            \n",
    "        \n",
    "    def forward(self, spec, pos_enc, attn_mask):\n",
    "                \n",
    "        split_spec= self.split(spec)\n",
    "        split_pos_enc= self.split(pos_enc)\n",
    "        split_attn_mask= self.split(attn_mask)\n",
    "        \n",
    "        pred_spec = []\n",
    "        \n",
    "        for a,b,c in zip(split_spec, split_pos_enc, split_attn_mask) :\n",
    "            \n",
    "            _pred_spec, _ = self.transformer(spec_input=a,\n",
    "                                        pos_enc=b,\n",
    "                                        mask_label=None,\n",
    "                                        attention_mask=c,\n",
    "                                        spec_label=None,\n",
    "                                        head_mask=None)\n",
    "            \n",
    "            pred_spec.append(_pred_spec)\n",
    "            \n",
    "        pred_spec= torch.cat(pred_spec, axis=1)\n",
    "        return pred_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram_to_numpy(spectrogram):\n",
    "    spectrogram = spectrogram.transpose(1, 0)\n",
    "    fig, ax = plt.subplots(figsize=(18, 3))\n",
    "    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\",cmap='magma')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    plt.xlabel(\"Frames\")\n",
    "    plt.ylabel(\"Channels\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.canvas.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path= '/home/jupyter/rfcx/rfcx/model_weights/pretrained_model/states-1000000.ckpt'\n",
    "\n",
    "ckpt_path= '/home/jupyter/rfcx/rfcx/model_weights/mockingjay_mel80_no_delta_cmvn_run4/states-2000.ckpt'\n",
    "# ckpt_path= '/home/jupyter/rfcx/rfcx/model_weights/mockingjay_mel80_no_delta_cmvn_run3/states-3000.ckpt'\n",
    "\n",
    "\n",
    "\n",
    "model , hidden_size, dr, device = load_model(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor= Preprocessor(hidden_size =768, dr=1, device=torch.device('cpu'))\n",
    "custom_model = CustomTransformerModel(model)\n",
    "custom_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-death",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-notebook",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file , input_file2 = audio_files[100] , audio_files[200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup= MixUp(load_mel.denoise, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1,sr= load_mel.load_audio(input_file, SAMPLE_RATE)\n",
    "show_audio(y1, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2,sr= load_mel.load_audio(input_file2, SAMPLE_RATE)\n",
    "show_audio(y2, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha,y3 = mixup(y1, y2 )\n",
    "print(alpha)\n",
    "\n",
    "show_audio(y3, SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y3= do_aug(y3, SAMPLE_RATE)\n",
    "show_audio(y3, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat= load_mel.get_spectrogram(y3,sr,apply_denoise=False,return_audio=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec= torch.tensor(feat)\n",
    "spec= spec.permute(1, 0)\n",
    "spec_stacked, pos_enc, attn_mask = preprocessor.process_MAM_data(spec=spec)\n",
    "\n",
    "\n",
    "spec_stacked.shape, pos_enc.shape, attn_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_spec= custom_model(spec_stacked, pos_enc, attn_mask)\n",
    "pred_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectrogram_to_numpy(pred_spec.detach().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-balance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
